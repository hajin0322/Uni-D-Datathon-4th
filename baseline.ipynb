{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision.transforms import CenterCrop, Resize\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':224,\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':16,\n",
    "    'SEED':42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocess(base_dir):\n",
    "    clean_dir = os.path.join(base_dir, 'clean')\n",
    "    noisy_dir = os.path.join(base_dir, 'noisy')\n",
    "\n",
    "    os.makedirs(clean_dir, exist_ok=True)\n",
    "    os.makedirs(noisy_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    source_dirs = []\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if 'GT' in dir_name:\n",
    "                source_dirs.append(os.path.join(root, dir_name))\n",
    "\n",
    "    if not source_dirs:\n",
    "        raise ValueError(\"No directory containing 'GT' found\")\n",
    "\n",
    "    for source_dir in source_dirs:\n",
    "        for filename in os.listdir(source_dir):\n",
    "            if filename.endswith('.jpg'):\n",
    "                shutil.move(os.path.join(source_dir, filename), os.path.join(clean_dir, filename))\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name not in ['clean', 'noisy'] and 'GT' not in dir_name:\n",
    "                current_dir = os.path.join(root, dir_name)\n",
    "                for filename in os.listdir(current_dir):\n",
    "                    if filename.endswith('.jpg'):\n",
    "                        shutil.move(os.path.join(current_dir, filename), os.path.join(noisy_dir, filename))\n",
    "                        \n",
    "    \n",
    "    for root, dirs, files in os.walk(base_dir, topdown=False):\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            if dir_name not in ['clean', 'noisy']:\n",
    "                shutil.rmtree(dir_path)\n",
    "                        \n",
    "    print('preprocessing done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './'\n",
    "training_base_dir = os.path.join(data_dir, 'Training')\n",
    "validation_base_dir = os.path.join(data_dir, 'Validation')\n",
    "\n",
    "Preprocess(training_base_dir)\n",
    "Preprocess(validation_base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, clean_image_paths, noisy_image_paths, transform=None):\n",
    "        self.clean_image_paths = [os.path.join(clean_image_paths, x) for x in os.listdir(clean_image_paths)]\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "        self.center_crop = CenterCrop(1080)\n",
    "        self.resize = Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE']))\n",
    "\n",
    "        # Create a list of (noisy, clean) pairs\n",
    "        self.noisy_clean_pairs = self._create_noisy_clean_pairs()\n",
    "\n",
    "    def _create_noisy_clean_pairs(self):\n",
    "        clean_to_noisy = {}\n",
    "        for clean_path in self.clean_image_paths:\n",
    "            clean_id = '_'.join(os.path.basename(clean_path).split('_')[:-1])\n",
    "            clean_to_noisy[clean_id] = clean_path\n",
    "        \n",
    "        noisy_clean_pairs = []\n",
    "        for noisy_path in self.noisy_image_paths:\n",
    "            noisy_id = '_'.join(os.path.basename(noisy_path).split('_')[:-1])\n",
    "            if noisy_id in clean_to_noisy:\n",
    "                clean_path = clean_to_noisy[noisy_id]\n",
    "                noisy_clean_pairs.append((noisy_path, clean_path))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        return noisy_clean_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_clean_pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path, clean_image_path = self.noisy_clean_pairs[index]\n",
    "\n",
    "        noisy_image = Image.open(noisy_image_path).convert(\"RGB\")\n",
    "        clean_image = Image.open(clean_image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Central Crop and Resize\n",
    "        noisy_image = self.center_crop(noisy_image)\n",
    "        clean_image = self.center_crop(clean_image)\n",
    "        noisy_image = self.resize(noisy_image)\n",
    "        clean_image = self.resize(clean_image)\n",
    "        \n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "            clean_image = self.transform(clean_image)\n",
    "        \n",
    "        return noisy_image, clean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDTA(nn.Module):\n",
    "    def __init__(self, channels, num_heads):\n",
    "        super(MDTA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
    "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)\n",
    "\n",
    "        q = q.reshape(b, self.num_heads, -1, h * w)\n",
    "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
    "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
    "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
    "\n",
    "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
    "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
    "        return out\n",
    "\n",
    "\n",
    "class GDFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super(GDFN, self).__init__()\n",
    "\n",
    "        hidden_channels = int(channels * expansion_factor)\n",
    "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
    "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
    "                              groups=hidden_channels * 2, bias=False)\n",
    "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
    "        x = self.project_out(F.gelu(x1) * x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads, expansion_factor):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = MDTA(channels, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.ffn = GDFN(channels, expansion_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                          .contiguous().reshape(b, c, h, w))\n",
    "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                         .contiguous().reshape(b, c, h, w))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Restormer(nn.Module):\n",
    "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[24, 48, 96, 192], num_refinement=4, expansion_factor=2.66):\n",
    "        \n",
    "        super(Restormer, self).__init__()\n",
    "\n",
    "        self.embed_conv = nn.Conv2d(3, channels[0], kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
    "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
    "                                       zip(num_blocks, num_heads, channels)])\n",
    "        \n",
    "        # the number of down sample or up sample == the number of encoder - 1\n",
    "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
    "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
    "\n",
    "        # the number of reduce block == the number of decoder - 1\n",
    "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
    "                                      for i in reversed(range(2, len(channels)))])\n",
    "        \n",
    "        # the number of decoder == the number of encoder - 1\n",
    "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
    "                                                       for _ in range(num_blocks[2])])])\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
    "                                             for _ in range(num_blocks[1])]))\n",
    "        \n",
    "        # the channel of last one is not change\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                             for _ in range(num_blocks[0])]))\n",
    "\n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                          for _ in range(num_refinement)])\n",
    "        self.output = nn.Conv2d(channels[1], 3, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fo = self.embed_conv(x)\n",
    "        out_enc1 = self.encoders[0](fo)\n",
    "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))\n",
    "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))\n",
    "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))\n",
    "\n",
    "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))\n",
    "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))\n",
    "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))\n",
    "        fr = self.refinement(fd)\n",
    "        out = self.output(fr) + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 시간 기록\n",
    "start_time = time.time()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 데이터셋 경로\n",
    "noisy_image_paths = './Training/noisy'\n",
    "clean_image_paths = './Training/clean'\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "train_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 커스텀 데이터셋 인스턴스 생성\n",
    "train_dataset = CustomDataset(clean_image_paths, noisy_image_paths, transform=train_transform)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "num_cores = os.cpu_count()\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], num_workers=int(num_cores/2), shuffle=True)\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Restormer 모델 인스턴스 생성 및 GPU로 이동\n",
    "model = Restormer().to(device)\n",
    "\n",
    "# 손실 함수와 최적화 알고리즘 설정\n",
    "optimizer = optim.AdamW(model.parameters(), lr = CFG['LEARNING_RATE'], weight_decay=1e-4)\n",
    "criterion = nn.L1Loss()\n",
    "scaler = GradScaler()\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CFG['EPOCHS'])\n",
    "\n",
    "# 모델의 파라미터 수 계산\n",
    "total_parameters = count_parameters(model)\n",
    "print(\"Total Parameters:\", total_parameters)\n",
    "\n",
    "# 모델 학습\n",
    "model.train()\n",
    "best_loss = 1000\n",
    "\n",
    "for epoch in range(CFG['EPOCHS']):\n",
    "    model.train()\n",
    "    epoch_start_time = time.time()\n",
    "    mse_running_loss = 0.0\n",
    "    \n",
    "    for noisy_images, clean_images in train_loader:\n",
    "        noisy_images = noisy_images.to(device)\n",
    "        clean_images = clean_images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(noisy_images)\n",
    "            mse_loss = criterion(outputs, clean_images)\n",
    "        \n",
    "        scaler.scale(mse_loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        \n",
    "        mse_running_loss += mse_loss.item() * noisy_images.size(0)\n",
    "\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "    epoch_end_time = time.time()\n",
    "    epoch_time = epoch_end_time - epoch_start_time\n",
    "    minutes = int(epoch_time // 60)\n",
    "    seconds = int(epoch_time % 60)\n",
    "    hours = int(minutes // 60)\n",
    "    minutes = int(minutes % 60)\n",
    "\n",
    "    mse_epoch_loss = mse_running_loss / len(train_dataset)\n",
    "    print(f\"Epoch {epoch+1}/{CFG['EPOCHS']}, MSE Loss: {mse_epoch_loss:.4f}, Lr: {current_lr:.8f}\")\n",
    "    print(f\"1epoch 훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")\n",
    "\n",
    "    if mse_epoch_loss < best_loss:\n",
    "        best_loss = mse_epoch_loss\n",
    "        torch.save(model.state_dict(), 'best_Restormer.pth')\n",
    "        print(f\"{epoch+1}epoch 모델 저장 완료\")\n",
    "\n",
    "# 종료 시간 기록\n",
    "end_time = time.time()\n",
    "\n",
    "# 소요 시간 계산\n",
    "training_time = end_time - start_time\n",
    "minutes = int(training_time // 60)\n",
    "seconds = int(training_time % 60)\n",
    "hours = int(minutes // 60)\n",
    "minutes = int(minutes % 60)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"훈련 소요 시간: {hours}시간 {minutes}분 {seconds}초\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetTest(data.Dataset):\n",
    "    def __init__(self, noisy_image_paths, transform=None):\n",
    "        self.noisy_image_paths = [os.path.join(noisy_image_paths, x) for x in os.listdir(noisy_image_paths)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        noisy_image_path = self.noisy_image_paths[index]\n",
    "        noisy_image = load_img(self.noisy_image_paths[index])\n",
    "        \n",
    "        # Convert numpy array to PIL image\n",
    "        if isinstance(noisy_image, np.ndarray):\n",
    "            noisy_image = Image.fromarray(noisy_image)\n",
    "\n",
    "        if self.transform:\n",
    "            noisy_image = self.transform(noisy_image)\n",
    "\n",
    "        return noisy_image, noisy_image_path\n",
    "\n",
    "\n",
    "test_transform = Compose([\n",
    "    ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def load_img(filepath):\n",
    "    img = cv2.imread(filepath)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "model = Restormer()\n",
    "model.load_state_dict(torch.load('best_Restormer.pth'))\n",
    "\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# 데이터셋 경로\n",
    "test_data_path = './open/test/Input'\n",
    "output_path = './open/test/submission'\n",
    "\n",
    "# 데이터셋 로드 및 전처리\n",
    "test_dataset = CustomDatasetTest(test_data_path, transform=test_transform)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# 이미지 denoising 및 저장\n",
    "for noisy_image, noisy_image_path in test_loader:\n",
    "    noisy_image = noisy_image.to(device)\n",
    "    denoised_image = model(noisy_image)\n",
    "    \n",
    "    # denoised_image를 CPU로 이동하여 이미지 저장\n",
    "    denoised_image = denoised_image.cpu().squeeze(0)\n",
    "    denoised_image = (denoised_image * 0.5 + 0.5).clamp(0, 1)\n",
    "    denoised_image = transforms.ToPILImage()(denoised_image)\n",
    "\n",
    "    # Save denoised image\n",
    "    output_filename = noisy_image_path[0]\n",
    "    denoised_filename = output_path + '/' + output_filename.split('/')[-1][:-4] + '.jpg'\n",
    "    denoised_image.save(denoised_filename) \n",
    "    \n",
    "    print(f'Saved denoised image: {denoised_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_folder(folder_path, output_zip):\n",
    "    shutil.make_archive(output_zip, 'zip', folder_path)\n",
    "    print(f\"Created {output_zip}.zip successfully.\")\n",
    "\n",
    "zip_folder(output_path, './submission')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
